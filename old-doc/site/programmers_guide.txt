h1. A Programmer's Guide to Fabric

This document is a secondary developer's attempt to explain how Fabric (sometimes "fab" or "Fab") is put together, and includes both statements (albeit tentative ones) and any questions/suggestions which arise during the author's exploration.

h2. Fabric basics

Fabric, at its core, executes one or more arbitrary Python functions, or *commands* (e.g. @deploy_my_site@ or @update_remote_svn@), which make use of Fabric-defined subroutines or *operations* (such as @run@ or @sudo@) to enact commands on remote Unix systems via SSH. Commands and operations are stored in global dictionaries (@COMMANDS@ and @OPERATIONS@ respectively) for ease of access and to keep them logically separate. These two dictionaries are populated by decorators, and thus are populated at module load time.

Commands and operations execute against a global *environment* specific to Fabric, an explicitly defined dictionary @ENV@ (as opposed to relying on e.g. @globals@). The environment provides context (remote username, what execution strategy to use, and so forth) as well as persistent storage for variables to be shared between commands.

h2. String interpolation

Throughout Fabric, there are two kinds of string interpolation, *eager* (@"&#37;(val)s"@) and *lazy* (@"$(val)"@). While eager syntax is proper Python interpolation syntax, if it is used _without_ the interpolation operator (@string % expression@) it will be replaced with the environment value, if it exists (i.e. as if given in the form @string % ENV@). Use of the @%@ operator is not overridden, however, so you can still generate strings with that syntax normally and Fabric will not interfere.

In typical usage, eager and lazy are generally indistinguishable, but lazy is useful when you want to define a variable placement in a string where that variable isn't set in @ENV@ until later on. This only really applies to @set@, as other operations will perform interpolation immediately, making the distinction between eager and lazy moot.

@_lazy_format@ (used on the input to most operations, and which does both types of interpolation) performs the eager substitution prior to performing lazy, so if for some reason one were to do the following:

bc[python]..
set(foo="bar", biz="$(foo)")
local("echo '%(biz)s'")

p. it would result in the command @echo 'bar'@.

h2. Users and passwords

As of Fabric 0.0.7, the system assumes a single, global username and password for all remote systems. The username defaults to your local system username, but may be overridden via @set@ (either at the top of a fabfile, or within commands). The password is not explicitly defined but must be entered at the first password prompt Fabric encounters (either at connection time, or at the first @sudo@ call if all connections use keyfiles). If the user encounters multiple password prompts during the connection phase, the last password entered becomes the final value of the global password.

Both of these values are set in stone after the connection phase is complete, meaning that if the user tries to connect to e.g. two different hosts with differing passwords or usernames, one of them will fail.

*Suggestion:* set up a 'registry' (probably modifying the structure of @ENV['fab_hosts']@ or @CONNECTIONS@) to keep track of distinct usernames and passwords per host. Obviously global defaults may still apply.

h2. Relationship between command functions

Multiple commands may be called in sequence by specifying them on the command line, e.g. @fab command1 command2@, in which case Fabric will execute them in order. Additionally, a command may explicitly call other commands as part of its sequence of operations, since all loaded commands become part of the builtin namespace (see below).

Commands may "communicate" by setting variables in the environment dictionary @ENV@, although locally defined Python variables will go out of scope once a given command ends. Therefore, commands called explicitly will inherit the local scope, but commands called together on the command line will only have @ENV@ in common.

Furthermore, at present, some global variables such as @ENV['fab_password']@ and @CONNECTIONS@ are set the first time connections are made to the remote servers, and will then stay the same through the rest of the session. In @_run_per_host@ (called for multi-host operations like @run@), there is a check for whether or not the global @CONNECTIONS@ list has been populated, and @_connect@ (which iterates over @ENV['fab_hosts']@) is only called if @CONNECTIONS@ is empty.

Therefore, it is not possible to update the list of hosts (by altering @ENV['fab_hosts']@) or the password to any useful effect. Take the following fabfile:

bc[python]..
def command1():
    set(fab_hosts=["foo.com"])
    run("rm /foo")

def command2():
    set(fab_hosts=["bar.com"])
    run("fm /bar")

p. and this usage of it:

bc. $ fab command1 command2

A user creating and running such a fabfile might expect to connect to @foo.com@ and delete @/foo@, then connect to @bar.com@ and delete @bar@. However, because connections are made only once (filling the @CONNECTIONS@ list), @command2@'s updating of @ENV['fab_hosts']@ will have no effect.

Similarly, @ENV['fab_password']@ is only stored once (as the last password entered during the connection phase) and will be used for all @sudo@ calls for all commands that follow, causing failure of one's remote passwords are not the same on all hosts.

*Question:* would it be worth re-connecting to the current list of hosts at the start of each command function? This would allow for more flexibility by re-defining @ENV['fab_hosts']@ (perform actions a, b on hosts 1, 2 and then perform action c on host 3) but would also be less efficient.

As stated in the TODO, we could also copy Capistrano's roles setup to some degree. Don't know offhand how they deal with command relationships and reconnecting - if command A is defined for hosts 1 and 2, and command B is defined for host 3, what happens when command A calls command B? I assume it would ignore command B's default host list and simply call B on hosts 1 and 2, but haven't checked.

h2. Fabric startup

h3. Pick fabfile to use

Fab, being a script on the user's @$PATH@ that just calls @fabric.main@, executes in the user's current working directory. It thus checks for the existence of a file in that current directory named something like <code>[fF]abfile(\.py)?</code> and stores that filename for use in loading user-defined commands (see below).

h3. Load user-specific Fabric env variables

If the file @$HOME/.fabric@ exists, Fab loads it up and appends any @key = value@ lines as pairs to the Fabric environment. Note that spacing within a line doesn't matter: @key=value@ is valid, as is @key = value@. Both keys and values are stored as strings: no inference is made as to the intended type, so e.g. @foo = 3@ will not result in an integer value.

*Question:* nitpicky, but why not swap the @_pick_fabfile@ and @_load_default_settings@ lines in @main@? They don't rely on each other and it just reads better to go 'load settings, pick fabfile, load fabfile'.

h3. Load commands from picked fabfile, or die trying

If the file picked in @_pick_fabfile@ exists, it is executed verbatim via @execfile@ in order to bring any functions within the fabfile into @locals@. @locals@ is then searched for non-"private" (underscore-prefixed) functions, which are added to @COMMANDS@, and any non-private objects (functions or otherwise) are added to <code>==__builtins__==</code> (so they may be referenced within commands as if they were built-ins).

If the picked file does not actually exist (meaning @_pick_fabfile@ gave up and just returned the first filename in its search list) Fabric will warn the user but continues executing in case the user specified a built-in command such as @help@.

h3. Parse @fab@ arguments

Fab can be called with a couple of different argument patterns, all of which build upon one another. The simplest is to specify a single command name:

bc. $ fab command

Multiple commands can be specified (and each of the following variants can be repeated in this manner, as well):

bc. $ fab command1 command2

Commands may be parameterized with a colon to provide arguments, which are turned into keyword arguments to the command function:

bc. $ fab command:arg=val

Values may be omitted, in which case they are None, equivalent to a call like @command(arg=None)@:

bc. $ fab command:arg

Command arguments may utilize string interpolation to substitute values from @ENV@, either eager or lazy, and they will work as expected (again, see tutorial for the difference between the two). Users must, of course, quote and/or escape the command-argument strings to prevent the shell from messing things up, e.g.:

bc. $ fab "command1:arg=%(val)s" "command2:arg=\$(val)"

Finally, note that the argument-parsing function, @_parse_args@, returns a list of @(command, arguments)@ tuples, where @command@ is the command name and @arguments@ is a dictionary of kwargs for that command.

h3. Validate commands

The parsed commands are checked to see if the list is empty (in which case a list of valid commands is printed and Fabric exits) or if any commands do not exist (in which case Fabric displays an error and exits). This has the resulting behavior of Fabric only executing if all specified commands are valid; it will not run some and error on others, but will fail completely. This is probably a good thing :)

h3. Execute!

Each command/arguments tuple generated by the argument-parsing step is run in order: the function object in @COMMANDS@ is executed with the associated set of arguments, if any (which are parsed for string interpolation at this point in time). At this point, assuming the command(s) are in the user-defined fabfile, those command functions are executed as normal Python.

Since user-defined commands have been "imported" into the main Fabric file via @execfile@, they have access to the local namespace, which is the key to the "flat" namespace users expect when writing their commands.

h2. Strategies

At the time of writing, Fabric employs the concept of *strategies*, which are functions defining how a multi-host operation (such as @run@ or @sudo@) is executed. A multi-host operation - denoted by the <code>==@run_per_host==</code> decorator - will execute once for each host defined in @ENV['fab_hosts']@, but how exactly this is done depends on the strategy.

For the purposes of this section, assume the following abstract fabfile:

bc[python]..
set(fab_hosts=["1", "2", "3"])

def command():
    run("a")
    run("b")
    run("c")

h3. Rolling

The only working strategy right now is *rolling*, meaning that Fabric simply executes operations serially, by iterating over each host connection and running the operation on them in order:

* a runs on 1
* a runs on 2
* a runs on 3
* b runs on 1
* b runs on 2
* b runs on 3
* c runs on 1
* c runs on 2
* c runs on 3

This approach is the simplest to implement but will also take the longest to run on average. It will also exhibit "blocking" behavior when, say, 'b' takes a long time to run on server 1 - this will delay execution of 'b' on servers 2 and 3, as well as all execution of 'c'.

h3. Fanout

An implemented-but-broken alternate strategy is *fanout*, meaning operations execute in parallel, by generating client-side threads (one per server per command) and then kicking them off in rapid succession:

* a runs on 1, a runs on 2, a runs on 3
* b runs on 1, b runs on 2, b runs on 3
* c runs on 1, c runs on 2, c runs on 3

Fanout will take less time to run, on average, and mitigates blockage a little bit - if 'b' takes a long time to run on server 1, it will still hold up execution of 'c' on all servers, but at least it won't hold up 'b' on servers 2 and 3.
    

h3. Proposed alternate strategy

The previous two strategies work fine in the case where one's commands do not need logic to operate, but which are a simple series of declarative statements like the example command above. However, if one wants to get more creative - like, say, branching based on the success or failure of a @run@ operation - the current model breaks down.

This is because Fabric's existing strategy for multi-host execution is "broad" - it runs 'a' on all hosts, then 'b' on all hosts - instead of "deep" - running 'a', then 'b' on host 1, 'a' then 'b' on host 2, and so forth. One could also use the terms "parallel" and "serial" to describe things:

* Rolling executes each operation serially (one host at a time) and executes operations themselves serially - *serial-serial*.
* Fanout executes each operation in parallel (all hosts at once) and executes operations serially - *parallel-serial*.

The "opposite" of these approaches would execute commands in parallel, i.e. the function that is a Fabric command would run once per host, with individual operations (@run@, @sudo@) only running against that single host each time. A naive approach would be to run the command itself serially, i.e. run the whole command on host 1, then host 2; this could be called *serial-parallel*. The "best" approach would be to thread the commands (as in fanout mode) so that they all execute in parallel; this would be 100% parallel and would thus be *parallel-parallel*.

Doing things in the current manner (==*-serial==) is easier than implementing the alternative (==*-parallel==), and in fact Capistrano - one of the inspirations for Fabric - suffers from the same "problem".

h4. Theoretical comparison to rolling and fanout

Putting aside implementation for a moment, this is how a serial-parallel strategy would fare with our example fabfile command:

* a runs on 1
* b runs on 1
* c runs on 1
* a runs on 2
* b runs on 2
* c runs on 2
* a runs on 3
* b runs on 3
* c runs on 3

This is clearly no more efficient than serial-serial, as it will take the exact same amount of time, and any given operation/host combination is capable of blocking everything that follows it. The only difference is that it enables logical flow to operate on a per-host basis.

The parallel-parallel strategy would use threads and look somewhat similar to fanout (parallel-serial):

* a runs on 1, a runs on 2, a runs on 3
* b runs on 1, b runs on 2, b runs on 3
* c runs on 1, c runs on 2, c runs on 3

except for a major difference, namely that the three line-items are not actually blocking on one another - if 'a' finishes on host 1 before the 'a' on host 2 finishes, host 1 continues on to operation 'b'. Thus the only blocking would be within a single host's execution of the command, as one would expect.

h4. Arguments for/against this approach

The main benefit of implementing ==*-parallel== execution is that it would allow these tools to become much more powerful in practice - one could effectively write a single script on their workstation and run it against an arbitrary host list. The only way to effect such behavior with ==*-serial== is to write out an actual script file to disk, push it to each remote machine, and then execute it via the remote machine's shell - a comparatively messy operation.

The main drawback of *-parallel is simply the inherent complexity of trying to run a single script on multiple different environments: one has to compensate for differing binary arguments, different locations of files, and so forth. Other tools exist that attempt to systematically solve that problem, such as Puppet; however, these tools require more up-front investment in order to use, and (as far as the author of this document knows) provide less flexibility/development speed than simple Python scripts, a tradeoff that doesn't work well for minor or esoteric tasks.

h2. Operations

List of operations (@run@, @sudo@, @put@, etc) and notes on them here.

h2. Commands

List of commands (@help@, @list@, etc) and notes on them here.